version: "3.9"
services:
  video-translator:
    image: zeroprg/video-translator
    ports:
      - "7860:7860"
      - "8081:8081"
    volumes:
      - ./app/downloads:/app/downloads
      - ./app/translations:/app/translations  
    environment:
      - LANG=C.UTF-8 
      - DOWNLOAD_FOLDER=/app/downloads
      - TRANSLATIONS_FOLDER=/app/translations
      - OLLAMA_URL=http://localhost:11434
      - TTS_URL=http://openedai-speech:8000
    networks:
      - my-network
    network_mode: host

#
#  ollama:
#    image: ollama/ollama
#    ports:
#      - "11434:11434"
#    volumes:
#      - ollama:/root/.ollama
#    environment:
#      - MODEL_NAME=llama2  # Assuming Ollama uses an environment variable to select the model
  
#    deploy:
#      resources:
#        reservations:
#          devices:
#            - driver: nvidia
#              count: all
#              capabilities: [gpu]
#    networks:
#      - my-network

  openedai-speech:
    build:
      context: ../openedai-speech  # Adjust if necessary to point to the directory containing the Dockerfile
      dockerfile: Dockerfile  # Or Dockerfile.min, depending on your requirements
    stdin_open: true
    tty: true
    ports:
      - "8000:8000"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    networks:
      - my-network

networks:
  my-network:
    driver: bridge

#volumes:
#  ollama:
